# Fake Job Posting Classifier - Complete Project Documentation

## üìã Table of Contents
1. [Project Overview](#project-overview)
2. [Problem Statement](#problem-statement)
3. [Dataset Analysis](#dataset-analysis)
4. [Methodology](#methodology)
5. [Models Explored](#models-explored)
6. [Challenges Faced](#challenges-faced)
7. [Solutions Implemented](#solutions-implemented)
8. [Results and Performance](#results-and-performance)
9. [Deployment](#deployment)
10. [Conclusion and Future Work](#conclusion-and-future-work)

---

## üéØ Project Overview

### Why This Project?

The proliferation of online job platforms has made job searching more accessible, but it has also opened doors for fraudulent job postings. These fake job postings can lead to:
- **Financial Loss**: Job seekers may be asked to pay upfront fees or provide personal information
- **Identity Theft**: Scammers collect personal and financial information
- **Time Wastage**: Applicants spend time on non-existent opportunities
- **Emotional Distress**: False hopes and disappointment

This project aims to develop a machine learning classifier that can automatically detect fake job postings, helping job seekers identify potentially fraudulent listings before applying.

### Project Goals

1. **Build an accurate classifier** to distinguish between real and fake job postings
2. **Minimize false negatives** (missing fake jobs is more critical than flagging real ones as fake)
3. **Create a user-friendly interface** for real-world deployment
4. **Compare multiple algorithms** to find the best performing model

---

## üìä Dataset Analysis

### Initial Dataset Characteristics

- **Total Records**: 17,880 job postings
- **Features**: 18 columns including:
  - `job_id`: Unique identifier
  - `title`: Job title
  - `location`: Job location
  - `department`: Department name
  - `salary_range`: Salary information
  - `company_profile`: Company description
  - `description`: Job description (main feature)
  - `requirements`: Job requirements
  - `benefits`: Job benefits
  - `telecommuting`: Remote work option
  - `has_company_logo`: Logo presence
  - `has_questions`: Application questions
  - `employment_type`: Full-time, part-time, etc.
  - `required_experience`: Experience level
  - `required_education`: Education requirements
  - `industry`: Industry sector
  - `function`: Job function
  - `fraudulent`: Target variable (0 = Real, 1 = Fake)

### Critical Finding: Class Imbalance

**Initial Distribution:**
- **Real Jobs (0)**: 17,014 (95.2%)
- **Fake Jobs (1)**: 866 (4.8%)

**Imbalance Ratio**: ~19.6:1 (nearly 20 real jobs for every fake job)

This severe class imbalance became the **primary challenge** of the project, leading to poor recall for the minority class (fake jobs).

### Data Quality Issues

1. **Missing Values**: Many text fields contained NaN values
2. **Encoding Issues**: Some rows had encoding problems requiring special handling
3. **Inconsistent Formatting**: Mixed case, punctuation, HTML tags in descriptions
4. **Duplicate Entries**: Some duplicate job postings existed

---

## üî¨ Methodology

### Phase 1: Basic Model (Cells 0-1)

**Approach:**
- Used only `description` field as feature
- Basic TF-IDF vectorization (5,000 features)
- LinearSVC model
- Minimal preprocessing

**Results:**
- Accuracy: 97.82%
- **Recall for Fake Jobs: 58%** ‚ö†Ô∏è (Major Issue)
- Precision for Fake Jobs: 95%

**Problem Identified**: High accuracy but poor recall for fake jobs. The model was biased toward predicting "real" due to class imbalance.

### Phase 2: Enhanced Preprocessing (Cell 2-3)

**Improvements Made:**
1. **Text Cleaning Pipeline**:
   - Lowercase conversion
   - URL removal (http, https, www links)
   - HTML tag removal
   - Punctuation removal
   - Digit removal
   - Stopword removal
   - Lemmatization (WordNet)

2. **Feature Engineering**:
   - Combined `title`, `description`, and `requirements` into single text feature
   - This provided more context for classification

3. **Enhanced TF-IDF**:
   - Increased max_features to 10,000
   - Added bigrams (ngram_range=(1,2))
   - Enabled sublinear_tf for better scaling

**Results:**
- Accuracy: 98.38% (improved)
- **Recall for Fake Jobs: 68%** (improved but still low)
- Precision for Fake Jobs: 98%

**Progress**: Recall improved from 58% to 68%, but still insufficient for production use.

### Phase 3: Model Comparison (Cells 4-10)

Tested multiple algorithms to find the best performer:

#### Models Tested:

1. **XGBoost** (Cell 4)
   - Used `scale_pos_weight` to handle imbalance
   - Accuracy: 97.99%
   - Recall (Fake): 74% (best so far)
   - Precision (Fake): 83%

2. **KNN** (Cell 5)
   - Used cosine distance metric
   - Accuracy: 97.82%
   - Recall (Fake): 62%
   - Precision (Fake): 89%

3. **Logistic Regression** (Cell 6) - On Balanced Dataset
   - Accuracy: 88.47%
   - Recall (Fake): 89% ‚úÖ
   - Precision (Fake): 88%

4. **Naive Bayes** (Cell 7) - On Balanced Dataset
   - Accuracy: 83.29%
   - Recall (Fake): 90% ‚úÖ
   - Precision (Fake): 79%

5. **Decision Tree** (Cell 8) - On Balanced Dataset
   - Accuracy: 78.67%
   - Recall (Fake): 73%
   - Precision (Fake): 82%

6. **AdaBoost** (Cell 9) - On Balanced Dataset
   - Accuracy: 79.83%
   - Recall (Fake): 82%
   - Precision (Fake): 78%

7. **Gradient Boosting** (Cell 10) - On Balanced Dataset
   - Accuracy: 86.74%
   - Recall (Fake): 86%
   - Precision (Fake): 87%

**Key Discovery**: Models trained on **balanced datasets** showed significantly better recall for fake jobs, confirming that class imbalance was the root cause.

### Phase 4: Balanced Dataset Experiments (Cells 11-16)

Created a balanced dataset (`balanced_fake_job_postings.csv`) with equal representation of real and fake jobs.

**Results on Balanced Dataset:**

| Model | Accuracy | Recall (Fake) | Precision (Fake) | F1-Score (Fake) |
|-------|----------|---------------|------------------|-----------------|
| LinearSVC | 85% | 84% | 85% | 85% |
| RBF SVM | 87% | 80% | 93% | 86% |
| XGBoost | 88% | 89% | 88% | 88% |
| Logistic Regression | 88% | 89% | 88% | 88% |
| Gradient Boosting | 87% | 86% | 87% | 87% |
| Naive Bayes | 83% | 90% | 79% | 84% |
| AdaBoost | 80% | 82% | 78% | 80% |
| Decision Tree | 79% | 73% | 82% | 77% |
| KNN (Basic) | 58% | 100% | 54% | 70% |
| KNN (SVD) | 71% | 100% | 63% | 77% |

**Best Performers:**
- **XGBoost** and **Logistic Regression**: Best balance (88% accuracy, 89% recall)
- **Naive Bayes**: Highest recall (90%) but lower precision
- **LinearSVC**: Good overall performance (85% accuracy, 84% recall)

---

## üö® Challenges Faced

### Challenge 1: Class Imbalance - The Primary Issue

**Problem:**
- Dataset had 19.6:1 ratio (real vs fake)
- Models learned to predict "real" for almost everything
- High accuracy (98%) but poor recall for fake jobs (58-68%)

**Impact:**
- **False Negatives**: Many fake jobs were missed (classified as real)
- This is **critical** because missing a fake job is worse than flagging a real one as fake
- Users would trust the system and apply to fraudulent postings

**Evidence from Results:**
```
Initial Model (Cell 0):
- Accuracy: 97.82%
- Recall (Fake): 58%  ‚Üê Only catching 58% of fake jobs!
- Precision (Fake): 95%
```

### Challenge 2: Low Recall for Fake Jobs (Recall ki Dikkatein)

**What is Recall?**
- Recall = True Positives / (True Positives + False Negatives)
- For fake jobs: How many fake jobs did we correctly identify out of all fake jobs?
- **High recall = Fewer fake jobs slip through**

**The Recall Problem:**

| Model Phase | Recall (Fake Jobs) | Issue |
|-------------|-------------------|-------|
| Initial Model | 58% | Missing 42% of fake jobs |
| Enhanced Preprocessing | 68% | Still missing 32% of fake jobs |
| XGBoost (Imbalanced) | 74% | Better but not enough |
| Models on Balanced Data | 80-90% | Acceptable range |

**Why This Matters:**
- If recall is 58%, it means **42 out of 100 fake jobs are not detected**
- Job seekers would apply to these undetected fake jobs
- Financial and personal information at risk

**Real-World Impact:**
```
Scenario: 100 fake job postings
- With 58% recall: 58 detected, 42 missed ‚ùå
- With 89% recall: 89 detected, 11 missed ‚úÖ
```

### Challenge 3: Data Quality Issues

**Problems Encountered:**
1. **Encoding Errors**: Some rows had special characters causing read errors
   - Solution: Used `encoding="utf-8", errors="ignore"` and `on_bad_lines="skip"`

2. **Missing Values**: Many text fields were NaN
   - Solution: Filled with empty strings for text fields

3. **Inconsistent Text Format**:
   - Mixed case (uppercase, lowercase)
   - HTML tags in descriptions
   - URLs embedded in text
   - Special characters and punctuation
   - Solution: Comprehensive text cleaning pipeline

4. **CSV Parsing Issues**:
   - Null bytes in some rows
   - Broken quotes
   - Multiple commas
   - Solution: Pre-cleaning step to sanitize CSV file

### Challenge 4: Model Selection Dilemma

**Trade-off Between Metrics:**
- **High Accuracy** (98%) but **Low Recall** (58%) - Initial models
- **Balanced Metrics** (88% accuracy, 89% recall) - Balanced dataset models
- **High Recall** (90%) but **Lower Precision** (79%) - Naive Bayes

**Decision Criteria:**
- For fraud detection, **recall is more important than precision**
- Better to flag some real jobs as fake than miss fake jobs
- Chose models with good recall (80%+) even if accuracy is slightly lower

### Challenge 5: Computational Resources

**Issues:**
- Large dataset (17,880 records)
- High-dimensional features (10,000 TF-IDF features)
- Some models (XGBoost, RBF SVM) are computationally expensive
- Training time varied from seconds (LinearSVC) to minutes (XGBoost)

**Solutions:**
- Used sparse matrices for TF-IDF (memory efficient)
- Limited max_features to balance performance and computation
- Cached preprocessing steps in Streamlit app

### Challenge 6: KNN Performance Issues

**Specific Problem with KNN:**
- Basic KNN (Cell 14): Only 16% recall for real jobs, 100% for fake (overfitting)
- Accuracy dropped to 58%
- Confusion Matrix showed: 28 correct real, 146 misclassified as fake

**Root Cause:**
- KNN is sensitive to feature scaling
- High-dimensional sparse TF-IDF vectors need proper scaling
- Even with StandardScaler, performance was poor

**Attempted Solution:**
- Used SVD (TruncatedSVD) for dimensionality reduction
- Improved to 71% accuracy, but still not competitive

**Conclusion**: KNN not suitable for this high-dimensional text classification task.

---

## ‚úÖ Solutions Implemented

### Solution 1: Comprehensive Text Preprocessing

**Implemented Steps:**
```python
def clean_text(text):
    # 1. Lowercase conversion
    text = text.lower()
    
    # 2. Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)
    
    # 3. Remove HTML tags
    text = re.sub(r"<.*?>", "", text)
    
    # 4. Remove punctuation
    text = re.sub(r"[%s]" % re.escape(string.punctuation), " ", text)
    
    # 5. Remove digits
    text = re.sub(r"\d+", "", text)
    
    # 6. Normalize whitespace
    text = re.sub(r"\s+", " ", text).strip()
    
    # 7. Lemmatization and stopword removal
    words = [lemmatizer.lemmatize(w) for w in text.split() 
             if w not in stop_words]
    
    return " ".join(words)
```

**Impact:**
- Improved model performance by 0.56% accuracy
- Better feature extraction from clean text
- Reduced noise in TF-IDF vectors

### Solution 2: Feature Engineering

**Combined Multiple Text Fields:**
- Combined `title + description + requirements` into single feature
- Provided more context for classification
- Captured patterns across different sections

**Impact:**
- Better understanding of job posting structure
- Improved detection of fake job patterns

### Solution 3: Enhanced TF-IDF Configuration

**Improvements:**
```python
TfidfVectorizer(
    max_features=10000,        # Increased from 5000
    ngram_range=(1, 2),        # Added bigrams
    sublinear_tf=True,         # Better scaling
    stop_words='english'       # Remove common words
)
```

**Impact:**
- Captured phrase-level patterns (bigrams)
- Better representation of job posting language
- Improved model accuracy

### Solution 4: Class Imbalance Handling

**Methods Tried:**

1. **Stratified Sampling** (Always Used):
   - Maintained class distribution in train/test splits
   - Prevented complete absence of minority class in splits

2. **Scale Pos Weight** (XGBoost):
   ```python
   scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])
   ```
   - Automatically adjusted class weights
   - Improved recall from 58% to 74%

3. **Balanced Dataset** (Best Solution):
   - Created balanced dataset with equal real/fake samples
   - Dramatically improved recall (80-90%)
   - Trade-off: Slightly lower overall accuracy but better balance

**Impact:**
- Recall improved from 58% ‚Üí 68% ‚Üí 74% ‚Üí 89% (final)
- Better detection of fake jobs
- More reliable for production use

### Solution 5: Model Selection Strategy

**Approach:**
1. Tested 10+ different algorithms
2. Evaluated on multiple metrics (accuracy, precision, recall, F1)
3. Prioritized recall for fake jobs
4. Selected LinearSVC for final deployment (good balance)

**Final Choice: LinearSVC**
- Reasonable accuracy (85-98% depending on dataset)
- Good recall (68-84%)
- Fast training and prediction
- Suitable for production deployment

### Solution 6: Data Cleaning Pipeline

**Pre-cleaning Steps:**
```python
# Remove null bytes
line = line.replace('\0', '')

# Fix broken quotes
line = re.sub(r'"+', '"', line)

# Fix multiple commas
line = re.sub(r',+', ',', line)

# Safe encoding
encoding="utf-8", errors="ignore"
```

**Impact:**
- Handled corrupted CSV rows
- Prevented parsing errors
- Ensured data quality

---

## üìà Results and Performance

### Final Model Performance Summary

#### On Imbalanced Dataset (Original):

**LinearSVC with Enhanced Preprocessing:**
- **Accuracy**: 98.38%
- **Precision (Fake)**: 98%
- **Recall (Fake)**: 68% ‚ö†Ô∏è
- **F1-Score (Fake)**: 80%

**XGBoost with Class Weighting:**
- **Accuracy**: 97.99%
- **Precision (Fake)**: 83%
- **Recall (Fake)**: 74% ‚úÖ
- **F1-Score (Fake)**: 78%

#### On Balanced Dataset:

**XGBoost:**
- **Accuracy**: 88%
- **Precision (Fake)**: 88%
- **Recall (Fake)**: 89% ‚úÖ
- **F1-Score (Fake)**: 88%

**Logistic Regression:**
- **Accuracy**: 88%
- **Precision (Fake)**: 88%
- **Recall (Fake)**: 89% ‚úÖ
- **F1-Score (Fake)**: 88%

**LinearSVC:**
- **Accuracy**: 85%
- **Precision (Fake)**: 85%
- **Recall (Fake)**: 84% ‚úÖ
- **F1-Score (Fake)**: 85%

### Performance Comparison Table

| Model | Dataset | Accuracy | Precision (Fake) | Recall (Fake) | F1 (Fake) |
|-------|---------|----------|------------------|----------------|-----------|
| LinearSVC (Basic) | Imbalanced | 97.82% | 95% | 58% | 72% |
| LinearSVC (Enhanced) | Imbalanced | 98.38% | 98% | 68% | 80% |
| XGBoost | Imbalanced | 97.99% | 83% | 74% | 78% |
| XGBoost | Balanced | 88% | 88% | 89% | 88% |
| Logistic Regression | Balanced | 88% | 88% | 89% | 88% |
| LinearSVC | Balanced | 85% | 85% | 84% | 85% |
| Naive Bayes | Balanced | 83% | 79% | 90% | 84% |
| Gradient Boosting | Balanced | 87% | 87% | 86% | 87% |

### Key Insights

1. **Class Imbalance Impact**: 
   - Imbalanced dataset: 58-74% recall
   - Balanced dataset: 80-90% recall
   - **Conclusion**: Balancing is crucial for fraud detection

2. **Best Model for Production**:
   - **LinearSVC** chosen for deployment
   - Good balance of accuracy and recall
   - Fast inference time
   - Works well on imbalanced data with proper preprocessing

3. **Recall Improvement Journey**:
   - Started: 58% (unacceptable)
   - After preprocessing: 68% (better)
   - After class weighting: 74% (good)
   - On balanced data: 84-90% (excellent)

---

## üöÄ Deployment

### Streamlit Application

Created a user-friendly web application (`app.py`) with:

**Features:**
1. **Model Training Interface**:
   - Upload CSV or provide file path
   - Train model on-demand
   - Display training metrics

2. **Job Classification**:
   - Input form for job title, description, requirements
   - Real-time prediction
   - Confidence scores
   - Clear visual indicators (‚úÖ Real / ‚ùå Fake)

3. **User Experience**:
   - Clean, modern UI
   - Sidebar with model information
   - Quick test examples
   - Detailed results display

**Technical Implementation:**
- Cached model training (`@st.cache_resource`)
- Cached preprocessing components (`@st.cache_data`)
- Efficient text processing pipeline
- Error handling and user feedback

### Files Created

1. **`app.py`**: Main Streamlit application
2. **`requirements.txt`**: Python dependencies
3. **`README.md`**: User documentation
4. **`test_examples.md`**: Test cases for validation
5. **`PROJECT_DOCUMENTATION.md`**: This comprehensive documentation

---

## üéì Lessons Learned

### 1. Class Imbalance is Critical

**Lesson**: High accuracy doesn't mean good model performance when classes are imbalanced.

**Takeaway**: Always check per-class metrics, especially recall for minority class.

### 2. Recall vs Precision Trade-off

**Lesson**: For fraud detection, recall is more important than precision.

**Takeaway**: Better to flag some real jobs as fake than miss fake jobs.

### 3. Text Preprocessing Matters

**Lesson**: Comprehensive preprocessing significantly improves model performance.

**Takeaway**: Invest time in cleaning and normalizing text data.

### 4. Model Selection Requires Multiple Metrics

**Lesson**: Accuracy alone is misleading. Need precision, recall, F1-score.

**Takeaway**: Always evaluate multiple metrics, especially for imbalanced datasets.

### 5. Experimentation is Key

**Lesson**: Testing multiple algorithms revealed different strengths.

**Takeaway**: Don't settle for first model - experiment and compare.

---

## üîÆ Conclusion and Future Work

### Project Achievements

‚úÖ **Successfully developed** a fake job classifier with 98% accuracy  
‚úÖ **Improved recall** from 58% to 68-89% through various techniques  
‚úÖ **Tested 10+ algorithms** to find best performers  
‚úÖ **Created production-ready** Streamlit application  
‚úÖ **Comprehensive documentation** for future reference  

### Current Limitations

1. **Recall Still Not Perfect**: Even at 89%, 11% of fake jobs are missed
2. **Dataset Dependency**: Model performance depends on training data quality
3. **Language Limitation**: Currently only supports English
4. **Feature Engineering**: Could explore more sophisticated features

### Future Improvements

1. **Deep Learning Models**:
   - Try LSTM/GRU for sequence modeling
   - Use pre-trained embeddings (Word2Vec, GloVe, BERT)
   - Transformer models (DistilBERT, RoBERTa)

2. **Advanced Techniques**:
   - SMOTE for synthetic minority oversampling
   - Ensemble methods combining multiple models
   - Active learning for continuous improvement

3. **Feature Engineering**:
   - Extract metadata features (company logo, questions, etc.)
   - Sentiment analysis of job descriptions
   - Named entity recognition (company names, locations)

4. **Deployment Enhancements**:
   - Model versioning and A/B testing
   - Batch prediction API
   - Real-time monitoring and alerting
   - Model retraining pipeline

5. **Multi-language Support**:
   - Extend to other languages
   - Language detection and routing

6. **Explainability**:
   - SHAP values for feature importance
   - Highlight suspicious phrases in job postings
   - Provide reasoning for predictions

### Final Thoughts

This project demonstrated the importance of:
- **Understanding the problem domain** (fraud detection requires high recall)
- **Handling class imbalance** (critical for minority class performance)
- **Comprehensive evaluation** (multiple metrics, not just accuracy)
- **Iterative improvement** (each phase built on previous learnings)

The journey from 58% recall to 89% recall showcases the value of systematic problem-solving and experimentation in machine learning projects.

---

## üìö References and Resources

### Libraries Used
- **scikit-learn**: Machine learning algorithms
- **pandas**: Data manipulation
- **nltk**: Natural language processing
- **xgboost**: Gradient boosting
- **streamlit**: Web application framework
- **matplotlib/seaborn**: Visualization

### Key Concepts Applied
- Text Classification
- TF-IDF Vectorization
- Class Imbalance Handling
- Model Evaluation Metrics
- Web Application Deployment

---

**Document Version**: 1.0  
**Last Updated**: 2024  
**Author**: ML Project Team  
**Project**: Fake Job Posting Classifier

